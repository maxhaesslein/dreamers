#!/usr/bin/python


# The Dreamers v4, 2023
# by maxhaesslein, 2014-2023
# www.maxhaesslein.de


# dependencies: opencv
# sudo apt install python3-opencv
# sudo apt install xserver-xorg xinit
#
# start with:
# startx /home/mh/dreamers/dreamers

# autostart happens in ~/.config/systemd/user/dreamers.service


print("")
print("###############################")
print("")
print("    THE DREAMERS v.4")
print("")
#print("    q: quit and shutdown")
#print("    e: exit to console")
print("    esc: exit")
print("  space: fake onset")
print("      1: toggle effect 'colorMap'")
print("      2: toggle effect 'blur'")
print("      3: toggle effect 'edgeDetection'")
print("      4: toggle effect 'flip'")
print("      5: toggle effect 'sharpening'")
print("      6: toggle effect 'emboss'")
print("      7: toggle effect 'inverse'")
print("      8: toggle effect 'multiple'")
print("      0: toggle all effects'")
#print("    ,: decrease sensitivity")
#print("    .: increase sensitivity")
#print("    n: decrease speed")
#print("    m: increase speed")
#print("    s: sharpening overlay")
#print("    w: soundstream overlay")
#print("    p: pause rendering")
#print("    d: debug screen level")
#print("    v: verbose level")
print("")
print("###############################")


from queue import Queue
from threading import Thread
import numpy as np
import cv2
import time
import os
import random


options = {

    'debugFPS': True,
    'debugTiming': True,
    'debugStreams': True,
    'debugEffects': True,

    'verbose': True,

    'fps': 30,

    'screenWidth': 720,
    'screenHeight': 720,


    'videostreamCount': 32,
    'videostreamMaxQueueSize': 20,

    'videoProcessorTimeout': 0.05,


    'windowName': 'The Dreamers',

    'videoExtension': 'mp4',
    'videoPath': 'footage/',

    'colorMaps': ["AUTUMN", "BONE", "JET", "WINTER", "RAINBOW", "OCEAN", "SUMMER", "SPRING", "COOL", "HSV", "PINK", "HOT"],
}


if options['verbose']:
    print('-- options --')
    print(options)



# setup window
cv2.namedWindow( options['windowName'], cv2.WINDOW_NORMAL )
cv2.resizeWindow( options['windowName'], options['screenWidth'], options['screenHeight'] )
cv2.setWindowProperty( options['windowName'], cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN )



# display startup image
startup_image = np.zeros((options['screenWidth'], options['screenHeight'], 3), np.uint8)
font = cv2.FONT_HERSHEY_TRIPLEX # TODO: use custom font
text = "The Dreamers"
fontsize = 2
fontwidth = 3
textsize = cv2.getTextSize(text, font, fontsize, fontwidth)[0]
textX = int((options['screenWidth'] - textsize[0]) / 2)
textY = int((options['screenHeight'] - textsize[1]) / 2)
cv2.putText(startup_image, text, (textX, textY), font, fontsize, (185,107,250), fontwidth)
text = "loading ...".upper()
fontsize = 1
fontwidth = 1
textsize = cv2.getTextSize(text, font, fontsize, fontwidth)[0]
textX = int((options['screenWidth'] - textsize[0]) / 2)
textY = int((options['screenHeight'] - textsize[1]))
cv2.putText(startup_image, text, (textX, textY), font, fontsize, (255,255,255), fontwidth)
cv2.imshow(options['windowName'], startup_image)
cv2.waitKey(60)



# find videos:
videoFiles = [f for f in os.listdir(options['videoPath']) if os.path.isfile(os.path.join(options['videoPath'], f)) and f.lower().endswith(options['videoExtension'])]
if options['verbose']:
    print('-- video files --')
    print(str(len(videoFiles))+' videofiles')
    #print(videoFiles)




def drawDebugRect( im, x, y, w, h, p, c ):

    lineWidth = 1

    if p > 1.0:
        p = 1.0

    filledHeight = int(h * p)

    cv2.rectangle( im, (x,y), (x+w, y+h), c, lineWidth )
    cv2.rectangle( im, (x,y+h-filledHeight), (x+w, y+h), c, -1 )



class VideostreamProcessor():

    def __init__(self):

        self.running = True

        self.streams = []
        self.index = 0

        for i in range(options['videostreamCount']):
            self.streams.append( self.loadVideo() )


    def start(self):
        t = Thread(target=self.update, args=())
        t.daemon = True
        t.start()
        return self


    def loadVideo(self):

        videoFile = random.choice(videoFiles)
        videoIn = cv2.VideoCapture(options['videoPath']+videoFile)

        # we currently start always at the beginning of the video; seeking may add framedrops
        #maxFrameCount = videoIn.get(cv2.CAP_PROP_FRAME_COUNT)
        #if maxFrameCount > 100:
        #    startFrame = random.randint(0, maxFrameCount-100)
        #    videoIn.set( cv2.CAP_PROP_POS_FRAMES, startFrame )

        stream = {
            'queue': Queue(maxsize=options['videostreamMaxQueueSize']),
            'videoIn': videoIn,
        }

        return stream


    def update(self):

        while self.running:

            # first, fill up current stream
            currentStream = self.streams[self.index]
            while not currentStream['queue'].full():
                self.readFrameToQueue(currentStream)

            # then fill up rest:
            for stream in self.streams:
                self.readFrameToQueue(stream)

            time.sleep(options['videoProcessorTimeout'])


    def stop(self):
        self.running = False
        for stream in self.streams:
            stream['videoIn'].release()


    def readFrameToQueue(self, stream):

        if stream['queue'].full():
            #print('queue already full')
            return

        ret, frame = stream['videoIn'].read()

        if ret == False:
            # no more frames left, loop video
            stream['videoIn'].set(cv2.CAP_PROP_POS_FRAMES, 0)
            return

        stream['queue'].put(frame)


    def changeVideostreamIndex(self):

        randomIndex = random.randint(0, options['videostreamCount']-1)
        if randomIndex == self.index:
            # don't change to current stream, just abort here
            return

        self.index = randomIndex


    def changeVideostreamInput(self):

        randomIndex = random.randint(0, options['videostreamCount']-1)
        if randomIndex == self.index:
            # don't change current stream, just abort here
            return

        self.streams[randomIndex] = self.loadVideo()


    def getNextFrame(self):

        if not self.running:
            return false

        frame = self.streams[self.index]['queue'].get()

        return frame

    def getInfo(self):

        infos = []
        for stream in self.streams:
            infos.append(stream['queue'].qsize())

        return self.index, infos


class EffectProcessor():

    def __init__(self):

        self.kernels = {
            'edgedetection': np.array([ [-1,-1,-1], [-1, 8,-1], [-1,-1,-1] ]),
            'sharpening':    np.array([ [ 1,-3, 1], [-1, 9,-1], [-1,-3,-1] ]),
            'emboss':    np.array([ [-3, -3,-3], [-4,-4,-4], [ 7, 8, 7] ])
        }

        self.options = {
            'colorMap': 0,
            'blur': [0,0],
            'edgeDetection': 0,
            'flip': 0,
            'sharpening': 0,
            'emboss': 0,
            'inverse': 0,
            'multiple': 0,
        }

        self.active = {
            'colorMap': True,
            'blur': True,
            'edgeDetection': True,
            'flip': True,
            'sharpening': True,
            'emboss': True,
            'inverse': True,
            'multiple': True,
        }

        self.colorMaps = []
        for colorMap in options['colorMaps']:
            self.colorMaps.append(getattr(cv2, "COLORMAP_"+colorMap))

        gamma = 1.0 # 0.0 - 1.0 --> different gamma values
        invGamma = 1.0 / gamma
        inverseLut = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
        self.luts = {
            'inverse': inverseLut[::-1]
        }


    def apply(self, frame):


        return frame


    def toggle(self, effect, setToBool = 'none' ):

        if effect == 'all':
            setToBool = not self.active['colorMap'] # we use the value of the first effect to toggle
            for effect in self.active:
                self.toggle(effect, setToBool)
            return

        print('setToBool '+str(setToBool))

        if setToBool is 'none':
            # toggle
            self.active[effect] = not self.active[effect]
        else:
            # set to value
            self.active[effect] = setToBool

        if effect == 'blur':
            self.options[effect] = [0,0]
        else:
            self.options[effect] = 0

        if options['verbose']:
            print('set effect '+str(effect)+' to '+str(self.active[effect]))


    def getInfo(self):
        return self.active, self.options


debugFont = cv2.FONT_HERSHEY_COMPLEX_SMALL

fps = 0
target_time = current_time = time.time()
previous_time = time.time()+1
loop_delta = 1./options['fps']
sleep_time = 0

videostreams = VideostreamProcessor().start()
effects = EffectProcessor()


# main loop
while True:

    start_time = time.time()

    frame = videostreams.getNextFrame()

    # resize frame
    frameHeight, frameWidth, frameChannels = frame.shape
    targetWidth = options['screenWidth']
    targetHeight = int(targetWidth * frameHeight/frameWidth)
    if targetHeight < options['screenHeight']:
        targetHeight = options['screenHeight']
        targetWidth = int(targetHeight * frameWidth/frameHeight)

    marginX = int((targetWidth - options['screenWidth'])/2)
    marginY = int((targetHeight - options['screenHeight'])/2)

    im = cv2.resize( frame, (targetWidth, targetHeight) )

    im = im[marginY:marginY+options['screenHeight'], marginX:marginX+options['screenWidth']]

    im = effects.apply(im)

    if options['debugFPS']:
        color = (0,255,0)
        if fps >= 29 or fps <= 31:
            fps = 30

        if fps < 25:
            color = (255,0,0)
        elif fps < 30:
            color = (255,255,0)

        cv2.putText( im, str(fps), (10,20), debugFont, 1, color, 1, cv2.LINE_AA )

    if options['debugTiming']:

        maxTime = int(loop_delta*1000)

        color = (0,255,0)
        if sleep_time < maxTime * 0.2:
            color = (0,0,255)
        elif sleep_time < maxTime*0.4:
            color = (0,255,255)

        freePercent = (float(sleep_time)/float(maxTime))
        if freePercent < 0.1:
            freePercent = 0

        filledPercent = 1-freePercent

        w = 30
        h = 100
        x = options['screenWidth'] - w - 10
        y = options['screenHeight'] - h - 10

        drawDebugRect( im, x, y, w, h, filledPercent, color )

    if options['debugStreams']:
        streamIndex, queueSizes = videostreams.getInfo()

        w = 10
        h = 100
        x = 10
        y = options['screenHeight'] - h - 10

        for i, queueSize in enumerate(queueSizes):
            color = (0,255,0)
            if i == streamIndex:
                color = (0,0,255)
            filledPercent = float(queueSize/options['videostreamMaxQueueSize'])
            drawDebugRect( im, x, y, w, h, filledPercent, color )
            x += w+5

    if options['debugEffects']:
        activeEffects, effectOptions = effects.getInfo()

        w = 10
        h = 50
        x = 80
        y = 10

        for activeEffect in activeEffects:

            filledPercent = 0
            color = (0,0,0)

            if activeEffects[activeEffect]:
                color = (0,255,0)
                value = effectOptions[activeEffect] # TODO
                filledPercent = 1

            drawDebugRect( im, x, y, w, h, filledPercent, color )
            x += w+5


    cv2.imshow(options['windowName'], im)


    previous_time, current_time = current_time, time.time()
    fps = int(1./(current_time-previous_time))
    target_time += loop_delta
    sleep_time = int((target_time - current_time)*1000)
    if sleep_time < 1:
        sleep_time = 1

    key = cv2.waitKey(sleep_time)

    onset = False

    if key == 27: # escape
        break

    elif key == 32: # space
        onset = True

    elif key&0xFF == ord('i'):
        videostreams.changeVideostreamIndex()

    elif key&0xFF == ord('c'):
        videostreams.changeVideostreamInput()

    elif key&0xFF == ord('0'):
        effects.toggle('all')
    elif key&0xFF == ord('1'):
        effects.toggle('colorMap')
    elif key&0xFF == ord('2'):
        effects.toggle('blur')
    elif key&0xFF == ord('3'):
        effects.toggle('edgeDetection')
    elif key&0xFF == ord('4'):
        effects.toggle('flip')
    elif key&0xFF == ord('5'):
        effects.toggle('sharpening')
    elif key&0xFF == ord('6'):
        effects.toggle('emboss')
    elif key&0xFF == ord('7'):
        effects.toggle('inverse')
    elif key&0xFF == ord('8'):
        effects.toggle('multiple')

    if onset:
        # TODO: maybe change effects
        if random.randint(0,1) == 1:
            videostreams.changeVideostreamIndex()
        else:
            videostreams.changeVideostreamInput()


print('bye!')

videostreams.stop()
cv2.destroyAllWindows()

