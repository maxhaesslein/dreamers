#!/usr/bin/python


# The Dreamers v4, 2023
# by maxhaesslein, 2014-2023
# www.maxhaesslein.de


# dependencies: opencv
# sudo apt install python3-opencv
# sudo apt install xserver-xorg xinit
#
# start with:
# startx $HOME/dreamers/dreamers

# autostart happens in ~/.config/systemd/user/dreamers.service


print("")
print("###############################")
print("")
print("    THE DREAMERS v.4")
print("")
#print("    q: quit and shutdown")
print("    esc: exit to console")
print("  space: fake onset")
print('')
print("      1: toggle effect 'colorMap'")
print("      2: toggle effect 'blur'")
print("      3: toggle effect 'edgeDetection'")
print("      4: toggle effect 'flip'")
print("      5: toggle effect 'sharpening'")
print("      6: toggle effect 'emboss'")
print("      7: toggle effect 'inverse'")
print("      8: toggle effect 'multiple'")
print("      0: toggle all effects")
print('')
print("      q: toggle FPS debug display")
print("      w: toggle timing debug display")
print("      e: toggle videostreams debug display")
print("      r: toggle effects debug display")
print("      t: toggle audio debug display")
print("      z: toggle onset debug display")
print("      u: toggle bpm debug display")
print("      i: toggle loudness debug display")
print("      d: toggle all debug displays")
print('')
print("      ,: decrease sensitivity")
print("      .: increase sensitivity")
#print("    s: sharpening overlay")
#print("    p: pause rendering")
#print("    v: verbose level")
print("")
print("###############################")


from queue import Queue
from threading import Thread
import numpy as np
import cv2
import time
import os
import sys
import random
import math
import pyaudio
import aubio


options = {

    'debugFPS': True,
    'debugTiming': True,
    'debugStreams': False,
    'debugEffects': False,
    'debugAudio': False,
    'debugOnset': False,
    'debugBpm': True,
    'debugLoudness': False,

    'verbose': True,


    'fps': 25,
    'fpsSmoothing':  0.6, # this smoothes only the fps _debug display_, NOT the real fps

    'screenWidth': 1024,
    'screenHeight': 768,


    'effectsActive': True,
    'frameResize': True,
    'resizeFrameOnLoad': True,
    'resizeFrameBeforeEffect': True,
    'effectsMultithreaded': True,

    'grayscale': False,


    'videostreamCount': 20,
    'videostreamMaxQueueSize': 20,


    'sensitivity': 0.7, # lower means slower, higher means faster; 0.1 - 2.0

    'windowName': 'The Dreamers',

    'videoExtension': 'mp4',
    'videoPath': 'footage/',

    'debugFont': cv2.FONT_HERSHEY_COMPLEX_SMALL,

    'audioDevice': 'auto',
    'audioOnsetMethod': 'specflux', # see https://aubio.org/doc/latest/specdesc_8h.html
    'audioDisplayScaling': 10000,

    'initSleepTime': 5, # in seconds, sleep time after loading videostreams before starting main loop


    'effectsActiveList': {
        'colorMap': False,
        'blur': False,
        'edgeDetection': False,
        'flip': False,
        'sharpening': False,
        'emboss': False,
        'inverse': False,
        'multiple': True,
    },
    'colorMaps': ["AUTUMN", "BONE", "JET", "WINTER", "RAINBOW", "OCEAN", "SUMMER", "SPRING", "COOL", "HSV", "PINK", "HOT"],

    'fontFile': 'fonts/Averia_Serif_Libre/AveriaSerifLibre-Bold.ttf',
    'fontSize': 10,

}


# probabilites:
# the higher the number, the less likely the hit; 0 means hit every time
probabilities = {
    'changeVideoStream': 0, # DEBUG
    #'changeVideoStream': 18, # if this is too low, fps goes down
    #'repeatVideoStream': 25, # fires on changeVideoStream if there is at least a buffer worth of frames # TODO
    'changeColorMap': 20,
    'setBlur': 125,
    'setEdgeDetection': 160,
    'setFlip': 200,
    'unsetFlip': 40,
    'setSharpening': 110,
    'setEmboss': 125,
    'setInverse': 80,
    'unsetInverse': 12,
    'setMultiple': 100, #330,
    'unsetMultiple': 60
}

maxVals = {
    'blur': 30,
    'edgeDetection': 10,
    'sharpening': 20,
    'emboss': 10
}


# setup window
cv2.namedWindow( options['windowName'], cv2.WINDOW_NORMAL )
cv2.resizeWindow( options['windowName'], options['screenWidth'], options['screenHeight'] )
cv2.setWindowProperty( options['windowName'], cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN )


# setup custom font
customFont = False
if options['fontFile']:
    customFont = cv2.freetype.createFreeType2()
    customFont.loadFontData( fontFileName=options['fontFile'], id=0 )


# display startup image
startupImage = np.zeros((options['screenHeight'], options['screenWidth'], 3), np.uint8)
font = cv2.FONT_HERSHEY_TRIPLEX
text = "The Dreamers"
fontSize = 2
fontWidth = 3
color = (185,107,250)
if customFont:
    fontSize = fontSize * 40
    textSize = customFont.getTextSize(text, fontSize, fontWidth)[0]
    textX = int((options['screenWidth'] - textSize[0]) / 2)
    textY = int((options['screenHeight'] - textSize[1]) / 2)
    customFont.putText( img=startupImage, text=text, org=(textX, textY), fontHeight=fontSize, color=color, thickness=-1, line_type=cv2.LINE_AA, bottomLeftOrigin=True )
else:
    textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
    textX = int((options['screenWidth'] - textSize[0]) / 2)
    textY = int((options['screenHeight'] - textSize[1]) / 2)
    cv2.putText( img=startupImage, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=color, thickness=fontWidth, lineType=cv2.LINE_AA )

text = "loading ..."
fontSize = 1
fontWidth = 1
color = (255,255,255)
if customFont:
    fontSize = fontSize * 40
    textSize = customFont.getTextSize(text, fontSize, fontWidth)[0]
    textX = int((options['screenWidth'] - textSize[0]) / 2)
    textY = int((options['screenHeight'] - textSize[1]*2))
    customFont.putText( img=startupImage, text=text, org=(textX, textY), fontHeight=fontSize, color=color, thickness=-1, line_type=cv2.LINE_AA, bottomLeftOrigin=True )
else:
    textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
    textX = int((options['screenWidth'] - textSize[0]) / 2)
    textY = int((options['screenHeight'] - textSize[1]*2))
    cv2.putText( img=startupImage, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=color, thickness=fontWidth, lineType=cv2.LINE_AA )

cv2.imshow(options['windowName'], startupImage)
cv2.waitKey(60)


# get audio input options
pa = pyaudio.PyAudio()
if options['verbose']:
    print("listing microphones")
    for i in range(pa.get_device_count()):
        dev = pa.get_device_info_by_index(i)
        print(i,dev['name'],dev['maxInputChannels'])
        if options['audioDevice'] == 'auto' and dev['maxInputChannels'] > 0:
            options['audioDevice'] = i
            print('  automatically set audio device to '+str(i))
if pa.get_device_count() < options['audioDevice']:
    print('audio device index is out of range (found '+str(pa.get_device_count())+' audio devices, requested #'+str(options['audioDevice'])+')')
    sys.exit()
audioDevice = pa.get_device_info_by_index(options['audioDevice'])
if audioDevice['maxInputChannels'] <= 0:
    print('audio device has no input channels! aborting')
    print('(use "arecord -l" to check available devices)')
    sys.exit()
audioOptions = {
    'deviceIndex': audioDevice['index'],
    'sampleRate': int(audioDevice['defaultSampleRate']),
    'bufferSize': int(audioDevice['defaultSampleRate']/options['fps']),
    'inputChannels': 1,
    'format': pyaudio.paFloat32,
    'npFormat': np.int16,
    'onsetMethod': options['audioOnsetMethod']
}


# find videos:
videoFiles = [f for f in os.listdir(options['videoPath']) if os.path.isfile(os.path.join(options['videoPath'], f)) and f.lower().endswith(options['videoExtension'])]


if options['verbose']:
    print('')
    print('---------')
    print('')
    print('-- options --')
    print(options)
    print('')
    print('-- audio options --')
    print(audioOptions)
    print('')
    print('-- video files --')
    print(str(len(videoFiles))+' videos found')
    print('')
    print('---------')
    print('')


def drawDebugRect( im, x, y, w, h, p, c ):

    lineWidth = 1

    if p < 0.0:
        p = 0.0
    elif p > 1.0:
        p = 1.0

    filledHeight = int(h * p)

    cv2.rectangle( im, (x,y), (x+w, y+h), c, lineWidth )
    cv2.rectangle( im, (x,y+h-filledHeight), (x+w, y+h), c, -1 )



class VideostreamProcessor():

    def __init__(self):

        self.running = True
        self.fpsMeasurement = options['fps']

        self.streams = []
        self.index = 0

        for i in range(options['videostreamCount']):
            self.streams.append( self.loadVideo() )


    def start(self):
        t = Thread(target=self.update, args=())
        t.daemon = True
        t.start()
        return self


    def loadVideo(self):

        videoFile = random.choice(videoFiles)
        videoIn = cv2.VideoCapture(options['videoPath']+videoFile)

        # we currently start always at the beginning of the video; seeking may add framedrops
        #maxFrameCount = videoIn.get(cv2.CAP_PROP_FRAME_COUNT)
        #if maxFrameCount > 100:
        #    startFrame = random.randint(0, maxFrameCount-100)
        #    videoIn.set( cv2.CAP_PROP_POS_FRAMES, startFrame )

        stream = {
            'queue': Queue(maxsize=options['videostreamMaxQueueSize']),
            'videoIn': videoIn,
            'videofile': videoFile,
        }

        return stream


    def update(self):

        previousTime = currentTime = time.time()
        targetTime = 1./options['fps']

        while self.running:

            startTime = time.time()

            # first, load 2 frames of the current stream
            currentStream = self.streams[self.index]
            for i in range(2):
                if currentStream['queue'].full():
                    break
                self.readFrameToQueue(currentStream)

            # then fill up rest:
            for stream in self.streams:
                self.readFrameToQueue(stream)

            usedTime = time.time() - startTime
            timeLeft = targetTime-usedTime
            if timeLeft > 0.0:
                time.sleep(timeLeft)

            previousTime, currentTime = currentTime, time.time()
            fps = math.ceil(1./(currentTime-previousTime))
            self.fpsMeasurement = (self.fpsMeasurement * options['fpsSmoothing']) + (fps * (1.0-options['fpsSmoothing']))


    def stop(self):
        self.running = False
        for stream in self.streams:
            stream['videoIn'].release()


    def readFrameToQueue(self, stream):

        if stream['queue'].full():
            return

        ret, frame = stream['videoIn'].read()

        if ret == False:
            # no more frames left, loop video
            stream['videoIn'].set(cv2.CAP_PROP_POS_FRAMES, 0)
            return

        if options['grayscale']:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)

        if options['resizeFrameOnLoad']:
            frame = resizeFrame(frame)

        stream['queue'].put(frame)


    def changeVideostreamIndex(self):

        randomIndex = random.randint(0, options['videostreamCount']-1)
        while True:
            randomIndex += 1
            if randomIndex >= options['videostreamCount']:
                randomIndex = 0
            if randomIndex == self.index:
                continue
            if self.streams[randomIndex]['queue'].qsize() >= 5:
                # find a stream that has at least 5 frames in its queue, then break
                break

        self.index = randomIndex
        if options['verbose']: print('change videostream to #'+str(self.index)+' ('+str(self.streams[self.index]['videofile'])+')')



    def changeVideostreamInput(self, streamIndex = False):

        if streamIndex == False:
            streamIndex = random.randint(0, options['videostreamCount']-1)

        if streamIndex == self.index:
            # don't change current stream, just abort here
            return

        self.streams[streamIndex] = self.loadVideo()


    def getNextFrame(self, blocking = False):

        if not self.running:
            return false

        if self.streams[self.index]['queue'].empty():
            # force switch to other stream
            self.changeVideostreamIndex()

        frame = self.streams[self.index]['queue'].get(blocking)

        return frame


    def getInfo(self):

        infos = []
        for stream in self.streams:
            infos.append(stream['queue'].qsize())

        return self.index, infos, self.fpsMeasurement


    def onset(self):

        if not probabilityCheck('changeVideoStream'):
            return

        oldStream = self.index

        self.changeVideostreamIndex()

        self.changeVideostreamInput(oldStream)


class EffectProcessor():

    def __init__(self):

        self.queueInput = Queue(maxsize=1)
        self.queueOutput = Queue(maxsize=1)
        self.running = True
        self.fpsMeasurement = options['fps']

        self.kernels = {
            'edgeDetection': np.array([ [-1,-1,-1], [-1, 8,-1], [-1,-1,-1] ]),
            'sharpening':    np.array([ [ 1,-3, 1], [-1, 9,-1], [-1,-3,-1] ]),
            'emboss':    np.array([ [-3, -3,-3], [-4,-4,-4], [ 7, 8, 7] ])
        }

        self.options = {
            'colorMap': 0,
            'blur': [0,0],
            'edgeDetection': 0,
            'flip': 0,
            'sharpening': 0,
            'emboss': 0,
            'inverse': 0,
            'multiple': 0,
        }

        self.active = options['effectsActiveList']

        self.colorMaps = []
        for colorMap in options['colorMaps']:
            self.colorMaps.append(getattr(cv2, "COLORMAP_"+colorMap))

        gamma = 1.0 # 0.0 - 1.0 --> different gamma values
        invGamma = 1.0 / gamma
        inverseLut = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
        self.luts = {
            'inverse': inverseLut[::-1]
        }


    def start(self):
        t = Thread(target=self.update, args=())
        t.daemon = True
        t.start()
        return self


    def update(self):

        previousTime = currentTime = time.time()

        while self.running:
            frame = self.queueInput.get(True)

            frame = self.apply(frame)

            self.queueOutput.put(frame)

            previousTime, currentTime = currentTime, time.time()
            fps = math.ceil(1./(currentTime-previousTime))
            self.fpsMeasurement = (self.fpsMeasurement * options['fpsSmoothing']) + (fps * (1.0-options['fpsSmoothing']))



    def put(self, frame):
        self.queueInput.put(frame)


    def get(self):
        return self.queueOutput.get()


    def apply(self, frame):

        # inverse
        if self.active['inverse'] and self.options['inverse'] > 0:
            frame = cv2.LUT(frame, self.luts['inverse'])

        # flip
        if self.active['flip'] and self.options['flip'] > 0:
            frame = cv2.flip(frame, 1)

        # edge detection
        if self.active['edgeDetection'] and self.options['edgeDetection'] > 0:
            frame = cv2.filter2D(frame, -1, self.kernels['edgeDetection'])
            self.options['edgeDetection'] -= 1

        # blur image
        if self.active['blur'] and (self.options['blur'][0] > 1 or self.options['blur'][1] > 1):
            frame = cv2.blur(frame, (self.options['blur'][0], self.options['blur'][1]))
            self.options['blur'][0] -= 1
            if self.options['blur'][0] < 1: self.options['blur'][0] = 1
            self.options['blur'][1] -= 1
            if self.options['blur'][1] < 1: self.options['blur'][1] = 1

        # sharpening
        if self.active['sharpening'] and self.options['sharpening'] > 0:
            frame = cv2.filter2D(frame, -1, self.kernels['sharpening'])
            self.options['sharpening'] -= 1

        # emboss
        if self.active['emboss'] and self.options['emboss'] > 0:
            frame = cv2.filter2D(frame, -1, self.kernels['emboss'])
            self.options['emboss'] -= 1

        # show frame multiple times
        if self.active['multiple'] and self.options['multiple']:

            if self.options['multiple'] == 2:
                frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5) # resize two half the size
                frame = np.concatenate((frame, frame), axis=0) # stack vertically
                frame = np.concatenate((frame, frame), axis=1) # stack horizontally

            elif self.options['multiple'] == 3:
                frame = cv2.resize(frame, (0,0), fx=0.334, fy=0.334)
                frame = np.concatenate((frame, frame, frame), axis=0)
                frame = np.concatenate((frame, frame, frame), axis=1)
                frame = frame[0:options['screenHeight'], 0:options['screenWidth']]

            elif self.options['multiple'] == 4:
                frame = cv2.resize(frame, (0,0), fx=0.25, fy=0.25)
                frame = np.concatenate((frame, frame, frame, frame), axis=0)
                frame = np.concatenate((frame, frame, frame, frame), axis=1)

            elif self.options['multiple'] == 5:
                frame = cv2.resize(frame, (0,0), fx=0.2, fy=0.2)
                frame = np.concatenate((frame, frame, frame, frame, frame), axis=0)
                frame = np.concatenate((frame, frame, frame, frame, frame), axis=1)

            elif self.options['multiple'] == 6:
                frame = cv2.resize(frame, (0,0), fx=0.167, fy=0.167)
                frame = np.concatenate((frame, frame, frame, frame, frame, frame), axis=0)
                frame = np.concatenate((frame, frame, frame, frame, frame, frame), axis=1)
                frame = frame[0:options['screenHeight'], 0:options['screenWidth']]

        # tint image
        if self.active['colorMap'] and self.options['colorMap'] > 0:
            frame = cv2.applyColorMap(frame, self.colorMaps[self.options['colorMap']-1])

        return frame


    def activate(self, effect = False):

        if not effect:
            effect = random.choice(list(self.active))

        effectCheck = effect
        if effect == 'unFlip':
            effectCheck = 'flip'
        elif effect == 'unInverse':
            effectCheck = 'inverse'
        elif effect == 'unMultiple':
            effectCheck = 'multiple'

        if not self.active[effectCheck]:
            if options['verbose']:
                print(' (effect '+str(effect)+' is not active)')
            return

        if options['verbose']:
            print('activating effect '+str(effect))

        # set colormap
        if effect == 'colorMap':
            self.options['colorMap'] = random.randint(0, len(self.colorMaps))
            if options['verbose']: print('changed colorMap to '+str(self.options['colorMap']))

        # set blur
        elif effect == 'blur':
            self.options['blur'] = [random.randint(4, maxVals['blur']), random.randint(4, maxVals['blur'])]
            if options['verbose']: print('set blur to '+str(self.options['blur']))

        # set edge detection
        elif effect == 'edgeDetection':
            self.options['edgeDetection'] = random.randint(2, maxVals['edgeDetection'])
            if options['verbose']: print('set edge detection to '+str(self.options['edgeDetection']))

        # set flip
        elif effect == 'flip':
            self.options['flip'] = 1
            if options['verbose']: print('set image flip')
        # unset flip
        elif effect == 'unFlip':
            self.options['flip'] = 0
            if options['verbose']: print('reset image flip')

        # set sharpening
        elif effect == 'sharpening':
            self.options['sharpening'] = random.randint(2, maxVals['sharpening'])
            if options['verbose']: print('set sharpening to '+str(self.options['sharpening']))

        # set emboss
        elif effect == 'emboss':
            self.options['emboss'] = random.randint(2, maxVals['emboss'])
            if options['verbose']: print('set sharpening to '+str(self.options['emboss']))

        # set inverse
        elif effect == 'inverse':
            self.options['inverse'] = 1
            if options['verbose']: print('set inverse')
        # unset inverse
        elif effect == 'unInverse':
            self.options['inverse'] = 0
            if options['verbose']: print('reset inverse')

        # set multiple
        elif effect == 'multiple':
            if random.randint(0,10) > 7:
                self.options['multiple'] = random.randint(2, 3)
            else:
                self.options['multiple'] = random.randint(4, 6)
            if options['verbose']: print('set multiple to '+str(self.options['multiple']))
        # unset multiple
        elif effect == 'unMultiple':
            self.options['multiple'] = 0
            if options['verbose']: print('reset multiple')


    def toggle(self, effect, setToBool = 'none' ):

        if effect == 'all':
            setToBool = not self.active['colorMap'] # we use the value of the first effect to toggle
            for effect in self.active:
                self.toggle(effect, setToBool)
            return

        print('setToBool '+str(setToBool))

        if setToBool == 'none':
            # toggle
            self.active[effect] = not self.active[effect]
        else:
            # set to value
            self.active[effect] = setToBool

        if effect == 'blur':
            self.options[effect] = [0,0]
        else:
            self.options[effect] = 0

        if options['verbose']:
            print('set effect '+str(effect)+' to '+str(self.active[effect]))


    def onset(self):

        # set colormap
        if probabilityCheck('changeColorMap'):
            self.activate('colorMap')

        # set blur
        if probabilityCheck('setBlur'):
            self.activate('blur')

        # set edge detection
        if probabilityCheck('setEdgeDetection'):
            self.activate('edgeDetection')

        # set flip
        if self.options['flip'] and probabilityCheck('unsetFlip'):
            self.activate('unFlip')
        elif probabilityCheck('setFlip'):
            self.activate('flip')

        # set sharpening
        if probabilityCheck('setSharpening'):
            self.activate('sharpening')

        # set emboss
        if probabilityCheck('setEmboss'):
            self.activate('emboss')

        # set inverse
        if self.options['inverse'] and probabilityCheck('unsetInverse'):
            self.activate('unInverse')
        elif probabilityCheck('setInverse'):
            self.activate('inverse')

        # set multiple
        if self.options['multiple'] and probabilityCheck('unsetMultiple'):
            self.activate('unMultiple')
        elif probabilityCheck('setMultiple'):
            self.activate('multiple')



    def getInfo(self):
        return self.active, self.options, self.fpsMeasurement


    def stop(self):
        self.running = False


class AudioProcessor:
    def __init__(self):

        self.running = True
        self.queue = Queue()

        self.fpsMeasurement = options['fps']

        self.peakBuffer = np.zeros(options['fps'])
        self.peakBufferOnset = np.zeros(options['fps'])
        self.peakBufferIndex = 0
        self.loudness = 0

        win_s = audioOptions['bufferSize']*2
        hop_s = int(win_s // 2)

        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=audioOptions['format'],
                                    input=True,
                                    channels=audioOptions['inputChannels'],
                                    input_device_index=audioOptions['deviceIndex'],
                                    frames_per_buffer=audioOptions['bufferSize'],
                                    rate=audioOptions['sampleRate'])

        self.onsetDetection = aubio.onset(method=audioOptions['onsetMethod'], buf_size=win_s, hop_size=hop_s, samplerate=audioOptions['sampleRate'])
        self.tempoDetection = aubio.tempo(method='default', buf_size=win_s, hop_size=hop_s, samplerate=audioOptions['sampleRate'])

        if options['verbose']:
            print('start aubio onset and beat detection with buffersize: '+str(audioOptions['bufferSize'])+' / deviceid: '+str(audioOptions['deviceIndex'])+' / samplerate: '+str(audioOptions['sampleRate'])+' / win_s: '+str(win_s)+' / hop_s: '+str(hop_s)+' / onsetMethod: '+str(audioOptions['onsetMethod']))


    def start(self):
        t = Thread(target=self.update, args=())
        t.daemon = True
        t.start()
        return self


    def update(self):

        previousTime = currentTime = time.time()
        targetTime = 1./options['fps']

        while self.running:

            startTime = time.time()

            # available frames: self.stream.get_read_available()
            audiobuffer = self.stream.read(num_frames=audioOptions['bufferSize'], exception_on_overflow=False)
            signal = np.frombuffer(audiobuffer, dtype=np.float32)

            onset = False
            onsetDet = self.onsetDetection(signal)
            if onsetDet:
                onset = True

            tempo = self.tempoDetection(signal)
            if tempo:
                onset = True

            peak = np.abs(np.max(signal)-np.min(signal))*options['audioDisplayScaling']

            self.peakBuffer[self.peakBufferIndex] = peak
            self.peakBufferOnset[self.peakBufferIndex] = onset

            self.peakBufferIndex += 1
            if self.peakBufferIndex >= len(self.peakBuffer):
                self.peakBufferIndex = 0


            if onset:
                self.queue.put(True)

            usedTime = time.time() - startTime
            timeLeft = targetTime-usedTime
            if timeLeft < 0:
                timeLeft = 0

            time.sleep(timeLeft)

            previousTime, currentTime = currentTime, time.time()
            fps = math.ceil(1./(currentTime-previousTime))
            self.fpsMeasurement = (self.fpsMeasurement * options['fpsSmoothing']) + (fps * (1.0-options['fpsSmoothing']))


    def getInfo(self):
        return self.peakBuffer, self.peakBufferOnset, self.peakBufferIndex, self.fpsMeasurement


    def getLoudness(self):
        return np.mean(self.peakBuffer)


    def isOnset(self):

        # TODO: if last onset is way too long ago, force onset

        if self.queue.qsize() > 0:
            return self.queue.get()

        return False


    def stop(self):

        self.running = False

        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()



def probabilityCheck( key ):
    if random.randint(0, probabilities[key])+1 <= options['sensitivity']*10:
        return True
    else:
        return False


def debugDisplay( im, onset ):

    textX = 30

    if options['debugFPS']:
        color = (0,255,0)

        if fpsMeasurement < options['fps']*0.8:
            color = (0,0,255)
        elif fpsMeasurement < options['fps']*0.9:
            color = (0,255,255)

        text = 'Main FPS: '+str(int(fpsMeasurement))
        textsize = cv2.getTextSize(text, options['debugFont'], 1, 1)[0]
        cv2.putText( im, text, (options['screenWidth']-textsize[0]-10, textX), options['debugFont'], 1, color, 1, cv2.LINE_AA )

        textX += 30


    if options['effectsActive'] and options['debugEffects']:
        activeEffects, effectOptions, effectsFpsMeasurement = effects.getInfo()

        h = 50
        x = 10
        y = 10

        for key, val in effectOptions.items():
            text = key

            if activeEffects[key]:
                color = (255,255,255)
            else:
                color = (0,0,0)
                val = 0

            maxVal = 1

            w = 20
            tX = x

            if key == 'colorMap':
                maxVal = len(options['colorMaps'])
                text = 'col'
            elif key == 'blur':
                maxVal = maxVals['blur']
                if not activeEffects[key]:
                    val = (0,0)

                w = 15

                filledPercent = val[0]/maxVal
                drawDebugRect( im, x, y, w, h, filledPercent, color )
                x += w+2
                tX = x-int(w/2)
                val = val[1]

            elif key == 'edgeDetection':
                maxVal = maxVals['edgeDetection']
                text = 'edg'
            elif key == 'sharpening':
                maxVal = maxVals['sharpening']
                text = 'shrp'
            elif key == 'emboss':
                maxVal = maxVals['emboss']
                text = 'emb'
            elif key == 'inverse':
                text = 'inv'
            elif key == 'multiple':
                val -= 1
                if val < 0:
                    val = 0
                maxVal = 5
                text = 'mult'

            filledPercent = val/maxVal

            drawDebugRect( im, x, y, w, h, filledPercent, color )
            textsize = cv2.getTextSize(text, options['debugFont'], 0.4, 1)[0]
            tX += int(w/2)-int(textsize[0]/2)
            cv2.putText( im, text, (tX, y+h+8), options['debugFont'], 0.4, color, 1, cv2.LINE_AA )

            x += w+5


        if options['effectsMultithreaded']:
            color = (0,255,0)
            if effectsFpsMeasurement < options['fps']*0.8:
                color = (0,0,255)
            elif effectsFpsMeasurement < options['fps']*0.9:
                color = (0,255,255)
            effectsFpsMeasurement = str(int(effectsFpsMeasurement))
        else:
            color = (255,255,255)
            effectsFpsMeasurement = '**'

        text = 'Effects FPS: '+effectsFpsMeasurement
        textsize = cv2.getTextSize(text, options['debugFont'], 1, 1)[0]
        cv2.putText( im, text, (options['screenWidth']-textsize[0]-10, textX), options['debugFont'], 1, color, 1, cv2.LINE_AA )

        textX += 30


    if options['debugTiming']:

        frameTime = usedTime + additionalFrameTime

        color = (0,255,0)
        if frameTime > targetTime * 0.95:
            color = (0,0,255)
        elif frameTime > targetTime*0.8:
            color = (0,255,255)

        filledPercent = float(frameTime)/float(targetTime)

        w = 30
        h = 100
        x = options['screenWidth'] - w - 10
        y = options['screenHeight'] - h - 10

        drawDebugRect( im, x, y, w, h, filledPercent, color )

    if options['debugStreams']:
        streamIndex, queueSizes, streamsFpsMeasurement = videostreams.getInfo()

        w = 10
        h = 100
        x = 10
        y = options['screenHeight'] - h - 10

        for i, queueSize in enumerate(queueSizes):
            color = (0,255,0)
            if i == streamIndex:
                color = (0,0,255)
            filledPercent = float(queueSize/options['videostreamMaxQueueSize'])
            drawDebugRect( im, x, y, w, h, filledPercent, color )
            x += w+5

        color = (0,255,0)
        if streamsFpsMeasurement < options['fps']*0.8:
            color = (0,0,255)
        elif streamsFpsMeasurement < options['fps']*0.9:
            color = (0,255,255)

        text = 'VideoStreams FPS: '+str(int(streamsFpsMeasurement))
        textsize = cv2.getTextSize(text, options['debugFont'], 1, 1)[0]
        cv2.putText( im, text, (options['screenWidth']-textsize[0]-10, textX), options['debugFont'], 1, color, 1, cv2.LINE_AA )

        textX += 30


    if options['debugAudio']:

        data, onsetData, index, audioFpsMeasurement = audio.getInfo()
        index -= 1
        if index < 0:
            index = options['fps']-1

        w = math.ceil(options['screenWidth']/len(data))
        x = options['screenWidth']-w
        y = int(options['screenHeight'] / 2)

        for i in range(len(data)):
            h = int(data[index])
            o = onsetData[index]

            color = (255,255,255)
            if o:
                color = (255,0,0)

            cv2.rectangle( im, (x,y), (x+w, y-h), color, -1 )

            x += w

            index += 1
            if index >= len(data):
                index = 0
            if x >= options['screenWidth']:
                x = 0

        color = (0,255,0)
        if audioFpsMeasurement < options['fps']*0.8:
            color = (0,0,255)
        elif audioFpsMeasurement < options['fps']*0.9:
            color = (0,255,255)

        text = 'Audio FPS: '+str(int(audioFpsMeasurement))
        textsize = cv2.getTextSize(text, options['debugFont'], 1, 1)[0]
        cv2.putText( im, text, (options['screenWidth']-textsize[0]-10, textX), options['debugFont'], 1, color, 1, cv2.LINE_AA )

        textX += 30


    if options['debugOnset']:

        w = 100
        h = 100
        x = options['screenWidth'] - w - 70
        y = options['screenHeight'] - h - 10

        filledPercent = 0.0
        color = (255,255,255)
        if onset:
            filledPercent = 1.0
            color = (0,0,255)

        drawDebugRect( im, x, y, w, h, filledPercent, color )


    if options['debugBpm']:

        bpmInt = int(bpm)
        averageBpmInt = int(averageBpm)

        sep = '='
        if bpmInt < averageBpmInt:
            sep = '<'
        elif bpmInt > averageBpmInt:
            sep = '>'

        text = 'BPM: '+str(bpmInt)+' '+sep+' '+str(averageBpmInt)
        textsize = cv2.getTextSize(text, options['debugFont'], 1, 1)[0]
        cv2.putText( im, text, (options['screenWidth']-textsize[0]-10, textX), options['debugFont'], 1, (255,255,255), 1, cv2.LINE_AA )

        textX += 30


    if options['debugLoudness']:

        loudnessInt = int(loudness)
        averageLoudnessInt = int(averageLoudness)

        sep = '='
        if loudnessInt < averageLoudnessInt:
            sep = '<'
        elif loudnessInt > averageLoudnessInt:
            sep = '>'

        text = 'Loudness: '+str(loudnessInt)+' '+sep+' '+str(averageLoudnessInt)
        textsize = cv2.getTextSize(text, options['debugFont'], 1, 1)[0]
        cv2.putText( im, text, (options['screenWidth']-textsize[0]-10, textX), options['debugFont'], 1, (255,255,255), 1, cv2.LINE_AA )

        textX += 30


    return im


def resizeFrame( frame ):

    if not options['frameResize']:
        return frame

    frameHeight, frameWidth, frameChannels = frame.shape
    targetWidth = options['screenWidth']
    targetHeight = int(targetWidth * frameHeight/frameWidth)
    if targetHeight < options['screenHeight']:
        targetHeight = options['screenHeight']
        targetWidth = int(targetHeight * frameWidth/frameHeight)

    marginX = int((targetWidth - options['screenWidth'])/2)
    marginY = int((targetHeight - options['screenHeight'])/2)

    frame = cv2.resize( frame, (targetWidth, targetHeight) )
    frame = frame[marginY:marginY+options['screenHeight'], marginX:marginX+options['screenWidth']]

    return frame


def messagesDisplay( frame, messages ):

    textX = options['screenHeight'] - 10

    for message in messages:
        textsize = cv2.getTextSize(message, options['debugFont'], 1, 1)[0]
        cv2.putText( frame, message, (10, textX), options['debugFont'], 1, (255,255,255), 1, cv2.LINE_AA )
        textX -= 30

    return frame



targetTime = 1./options['fps']
previousTime = currentTime = time.time()
usedTime = 0

fpsMeasurement = options['fps']

videostreams = VideostreamProcessor().start()


if options['verbose']:
    print('init -- sleep for '+str(options['initSleepTime'])+' seconds')

time.sleep(options['initSleepTime'])

if options['effectsActive']:
    effects = EffectProcessor()
    if options['effectsMultithreaded']:
        effects.start()

audio = AudioProcessor().start()

time.sleep(0.5)

if options['effectsActive'] and options['effectsMultithreaded']:
    if not options['resizeFrameBeforeEffect']:
        effects.put(videostreams.getNextFrame(True))
    else:
        effects.put(startupImage)

frameNumber = 0
additionalFrameTime = 0

frameCountIn30Seconds = options['fps']*30
frameCountIn10Seconds = options['fps']*10

beatList = np.zeros(frameCountIn30Seconds)
bpm = 0
averageBpm = 0
bpmList = np.zeros(frameCountIn10Seconds)

loudness = 0
averageLoudness = 0
loudnessList = np.zeros(frameCountIn10Seconds)

messages = []

# main loop
while True:

    startTime = time.time()

    frame = videostreams.getNextFrame()
    if type(frame) == type(None): # no frame retreived, use last frame
        frame = lastFrame

    lastFrame = frame

    if not options['resizeFrameOnLoad'] and options['resizeFrameBeforeEffect']:
        frame = resizeFrame(frame)

    if options['effectsActive']:
        if options['effectsMultithreaded']:
            nextFrame = frame
            frame = effects.get()
            effects.put(nextFrame)
        else:
            frame = effects.apply(frame)

    if not options['resizeFrameOnLoad'] and not options['resizeFrameBeforeEffect']:
        frame = resizeFrame(frame)

    onset = audio.isOnset()
    loudness = audio.getLoudness()

    listIndex30 = frameNumber%frameCountIn30Seconds
    if onset:
        beatList[listIndex30] = 1
    else:
        beatList[listIndex30] = 0

    bpm = np.count_nonzero(beatList)*2 # beatList has a length of 30 seconds, so we multiply by 2 to get the beats per 60 seconds / 1 minute

    listIndex10 = frameNumber%frameCountIn10Seconds
    bpmList[listIndex10] = bpm
    loudnessList[listIndex10] = loudness

    averageLoudness = np.mean(loudnessList)
    averageBpm = np.mean(bpmList)


    frame = debugDisplay(frame, onset)

    if len(messages) > 0:
        frame = messagesDisplay(frame, messages)

        if frameNumber%(options['fps']/2) == 0: # TODO: this should be depending on when the message was added
            messages.pop(0)

    cv2.imshow(options['windowName'], frame)

    usedTime = time.time() - startTime
    timeLeft = targetTime-usedTime
    sleepTime = timeLeft - additionalFrameTime # this is the approximate time that the rest of this loop takes
    sleepTime = int(sleepTime*1000)
    if sleepTime < 1:
        sleepTime = 1

    key = cv2.waitKey(sleepTime)

    midTime = time.time()

    previousTime, currentTime = currentTime, time.time()
    fps = round(1./(currentTime-previousTime))
    fpsMeasurement = (fpsMeasurement * options['fpsSmoothing']) + (fps * (1.0-options['fpsSmoothing']))

    if key == 27: # escape
        messages.append('exiting')
        break

    elif key == 32: # space
        messages.append('fake beat')
        onset = True

    elif key&0xFF == ord('i'):
        messages.append('change random videostream index')
        videostreams.changeVideostreamIndex()

    elif key&0xFF == ord('c'):
        messages.append('change random videostream input')
        videostreams.changeVideostreamInput()

    elif key&0xFF == ord('0'):
        messages.append('toggle all effects')
        effects.toggle('all')
    elif key&0xFF == ord('1'):
        messages.append('toggle colorMap effect')
        effects.toggle('colorMap')
    elif key&0xFF == ord('2'):
        messages.append('toggle blur effect')
        effects.toggle('blur')
    elif key&0xFF == ord('3'):
        messages.append('toggle edgeDetection effect')
        effects.toggle('edgeDetection')
    elif key&0xFF == ord('4'):
        messages.append('toggle flip effect')
        effects.toggle('flip')
    elif key&0xFF == ord('5'):
        messages.append('toggle sharpening effect')
        effects.toggle('sharpening')
    elif key&0xFF == ord('6'):
        messages.append('toggle emboss effect')
        effects.toggle('emboss')
    elif key&0xFF == ord('7'):
        messages.append('toggle inverse effect')
        effects.toggle('inverse')
    elif key&0xFF == ord('8'):
        messages.append('toggle multiple effect')
        effects.toggle('multiple')

    elif key&0xFF == ord('q'):
        messages.append('toggle fps debug display')
        options['debugFPS'] = not options['debugFPS']
    elif key&0xFF == ord('w'):
        messages.append('toggle timing debug display')
        options['debugTiming'] = not options['debugTiming']
    elif key&0xFF == ord('e'):
        messages.append('toggle videostreams debug display')
        options['debugStreams'] = not options['debugStreams']
    elif key&0xFF == ord('r'):
        messages.append('toggle effects debug display')
        options['debugEffects'] = not options['debugEffects']
    elif key&0xFF == ord('t'):
        messages.append('toggle audio debug display')
        options['debugAudio'] = not options['debugAudio']
    elif key&0xFF == ord('z'):
        messages.append('toggle onset debug display')
        options['debugOnset'] = not options['debugOnset']
    elif key&0xFF == ord('u'):
        messages.append('toggle bpm debug display')
        options['debugBpm'] = not options['debugBpm']
    elif key&0xFF == ord('i'):
        messages.append('toggle loudness debug display')
        options['debugLoudness'] = not options['debugLoudness']

    elif key&0xFF == ord('d'):
        messages.append('toggle all debug displays')
        newBool = not options['debugFPS']
        options['debugFPS'] = newBool
        options['debugTiming'] = newBool
        options['debugStreams'] = newBool
        options['debugEffects'] = newBool
        options['debugAudio'] = newBool
        options['debugOnset'] = newBool
        options['debugBpm'] = newBool
        options['debugLoudness'] = newBool

    elif key&0xFF == ord(','):
        options['sensitivity'] -= 0.05
        if options['sensitivity'] < 0:
            options['sensitivity'] = 0.0
        if options['verbose']:
            print('set sensitivity to '+str(options['sensitivity']))
        messages.append('decrease sensitivity to '+str(options['sensitivity']))
    elif key&0xFF == ord('.'):
        options['sensitivity'] += 0.05
        if options['sensitivity'] > 1:
            options['sensitivity'] = 1.0
        if options['verbose']:
            print('set sensitivity to '+str(options['sensitivity']))
        messages.append('increase sensitivity to '+str(options['sensitivity']))

    if onset:
        videostreams.onset()

        if options['effectsActive']:
            effects.onset()

    frameNumber += 1

    additionalFrameTime = time.time() - midTime


if options['verbose']:
    print('shutting down ...')

if options['effectsActive'] and options['effectsMultithreaded']:
    effects.stop()

videostreams.stop()
audio.stop()

cv2.destroyAllWindows()

print('bye!')

